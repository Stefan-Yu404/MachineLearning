{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The packages for this file:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# template:\n",
    "temp = pd.DataFrame({\"Race\": [\"White\", \"White\", \"Asian\", \"Black\", \"Declined\", \"Asian\", \"Black\"], \n",
    "                    \"Time\": [\"11/04/2023\", \"12/04/2023\", \"13/04/2023\", \"14/04/2023\", \"15/04/2023\", \"16/04/2023\",\n",
    "                             \"17/04/2023\"], \n",
    "                    \"hoursStayICU\": [10, 20, 30, 40, 50, 60, 70]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Supervised Learning**: Given some data and some outcome, use the data to predict the outcome.  \n",
    "2. **Unsupervised Learning**: Use the data to learn some underlying latent structure. This structure can then be used to generate more data. It can also be used downstream in other types of learning. Note: There are no outcomes to predict in this case. \n",
    "3. **Reinforcement Learning**: Given a reward signal, tell me what actions I should take to maximize this signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What matters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Data**: in case of \"Garbage in, garbage out\".  \n",
    "2. **Models**: What kind of model you use, it will be the focus but before it please make sure the quality of the data.  \n",
    "3. **Criterion**: How do we say a model is good enouph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with Data cleaning. You need to understand the data in order to correctly select a machine learning model. The first step to understanding the dataset is importing it and then probing it to ask questions. This is **Exploratory Data Analysis** (EDA). When importing data, there are many things that can go wrong immediately. This results in the need for Data Clearing. The problems you may face to:  \n",
    "1. The data is malformed. \n",
    "2. There are missing values.\n",
    "3. The data has different scales.\n",
    "4. There are outliers.\n",
    "5. There is under-sampling or over-sampling.\n",
    "6. The data has mixed modalities, e.g. categorical and continuous values.  \n",
    "  \n",
    "(The first project is about EDA with the synthetic data of credit fraudulent data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 The data is malformed.  \n",
    "You can use a debugger and try to read the lines in one at a time. With each line, you can check that the\n",
    "imported values are within a standard range. If the line is malformed (i.e. a row has 6 columns and the rest of the file has 7), itâ€™s usually best to throw out the entire row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Missing values\n",
    "Intuitively, we lack the data.  \n",
    "1. Drop the samples with missing values. However, this may lead to the information loss. You may try it if the missing values are little and the dataset is mostly clean.  \n",
    "2. Impulation. \n",
    "    1. For the categorical ones, you can add a new tag like: \"Unknown\". For the continuous ones, there are a lot of ways, including simply replace them with 0, mean value or some special case (Like population in a state, we can impute it with average population density calculated multiply the area of the missing state).\n",
    "    2. Machine learning methods: \n",
    "        1. do K-means clustering on the data with the missing feature dimension removed. Assing the missing datapoint to a cluser and then impute with the average of the cluster.\n",
    "        2. Fit a linear model to do prediction \n",
    "        3. More complicate method: GAN (Generative Adversarial Neutral network). For example, for an image with a part of filtered, we can generate a complete one. **I may complement it later**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 The dataset has different scales\n",
    "Many machine learning models will be sensitive to this difference in scale. Some data may record the percentile information, some may record data in different units: 1 hundred, 1.4 million in different columns. \n",
    "Normalize them: \n",
    "1. divide by the maximum: pro: interpretable; con: get affected by the outlier.\n",
    "2. standardize it by $\\frac{x - \\bar{x}}{\\sigma_x}$ pro: affect less by outlier; con: less interprebable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5],\n",
       "       [-1. ],\n",
       "       [-0.5],\n",
       "       [ 0. ],\n",
       "       [ 0.5],\n",
       "       [ 1. ],\n",
       "       [ 1.5]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can calculate it by calculation and also use packages in sklearn\n",
    "scaler = StandardScaler()\n",
    "standardVariable = scaler.fit_transform(temp[[\"hoursStayICU\"]])\n",
    "standardVariable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 There are outliers  \n",
    "There are many contions and methods for outliers. \n",
    "1. Drop According to the characteristics of the variable, we can drop the data violates the ground-truth directly, we can also set a limit like the minimum for our data to drop. \n",
    "2. Statistical method: We can also visualiza it with histograms, scatter plots or using some statistical methods to find the influential points (like Cook's distance, DFFIT).\n",
    "3. Special attention: z-score, it is widely used that we use z-score = 3 to cut the data outside the range assuming it follows a Gaussian distribution.  \n",
    "4. For high-dimension data, z-score may not be able to use. We solve it with anomaly detection. **Flag it and complement it later**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 There is under-sampling or over-sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 The data has mixed modalities, e.g. categorical and continuous values.  \n",
    "\n",
    "There are many cases. Generally, the categorical, numerical data and time series data. You may try following codes:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Race    object\n",
      "Time    object\n",
      "dtype: object\n",
      "--------------------------------------------------\n",
      "   Asian  Black  Declined  White\n",
      "0      0      0         0      1\n",
      "1      0      0         0      1\n",
      "--------------------------------------------------\n",
      "datetime64[ns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/galahad/opt/anaconda3/lib/python3.9/site-packages/pandas/core/dtypes/cast.py:1163: UserWarning: Parsing '13/04/2023' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  to_datetime(arr).values,\n",
      "/Users/galahad/opt/anaconda3/lib/python3.9/site-packages/pandas/core/dtypes/cast.py:1163: UserWarning: Parsing '14/04/2023' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  to_datetime(arr).values,\n",
      "/Users/galahad/opt/anaconda3/lib/python3.9/site-packages/pandas/core/dtypes/cast.py:1163: UserWarning: Parsing '15/04/2023' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  to_datetime(arr).values,\n",
      "/Users/galahad/opt/anaconda3/lib/python3.9/site-packages/pandas/core/dtypes/cast.py:1163: UserWarning: Parsing '16/04/2023' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  to_datetime(arr).values,\n",
      "/Users/galahad/opt/anaconda3/lib/python3.9/site-packages/pandas/core/dtypes/cast.py:1163: UserWarning: Parsing '17/04/2023' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  to_datetime(arr).values,\n"
     ]
    }
   ],
   "source": [
    "print(temp.dtypes)\n",
    "print(\"--------------------------------------------------\")\n",
    "# 1. Turn categorical data to one-hot encoding\n",
    "oneHot = pd.get_dummies(temp['Race'])\n",
    "print(oneHot.head(2))\n",
    "print(\"--------------------------------------------------\")\n",
    "# 2. Turn time String to time format data\n",
    "time = temp[\"Time\"].astype(\"datetime64\") ## DD/MM/YYYY\n",
    "print(time.dtypes)\n",
    "# 3. The loss for continuous and categorical data are different, like MSE for cont. and CrossEntropy for categorical. \n",
    "#    It often happens when the predictive variables are not unique of different kinds. We may try predict them \n",
    "#    separately or add a weight: a*L_dis + (1 - a)*L_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
