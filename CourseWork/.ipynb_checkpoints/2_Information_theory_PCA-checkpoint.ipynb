{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "695e6587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The packages for this file:\n",
    "import numpy as np\n",
    "import pandas as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba617492",
   "metadata": {},
   "source": [
    "### Why we need Information Theory and PCA ?  \n",
    "Sometimes, we need to implement ML methods with high dimensional data, like images with shape: [1, 3, 256, 256], 1 stands for batch size, 3 stands for RGB, the image has $256*256$ pixels. If we treat each pixel as a faeture dimension, the dimension is too high to handle. Hence, we want to find the parts with the most information. It is just one of cases, called the curse of Dimensionality. There are two general cases that it happens:  \n",
    "1. There are too many data points (samples), too many rows.  \n",
    "2. There are too many features, too many columns.  \n",
    "  \n",
    "The first case can be addressed with clustering and unsupervised learning methods. As for the second case, we can use dimension reduction methods such as SVD, VAEs. We will discuss PCA today. To discuss it, basic information theory is necessay, we want to keep the features that contain the most information to fulfill the goals.  \n",
    "  \n",
    "In fact, information theory also shows up in random forests, decision trees, and in classification problems. Information theory is at the heart of machine learning so it’s really important.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f5f375",
   "metadata": {},
   "source": [
    "## 1. Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8708b707",
   "metadata": {},
   "source": [
    "Entropy measures the amount of uncertainty in a distribution, it quantifies it. There are four basic properties that is provied by Khinchin (1957, Mathematical Foundations of Information Theory):  \n",
    "1. Property I:  \n",
    "    1. The uniform distribution is maximally uncertain. \n",
    "    2. Given n different possible outcomes, you can argue that the highest level of uncertainty is achieved if we simply put maximal probability on each outcome. \n",
    "    3. You might be able to argue that a distribution with heavy tails is “more uncertain” because the total range is larger. \n",
    "    4. So you need to be careful to specify that you want maximal uncertainty over a fixed interval. Of course, increasing interval size will always increase uncertainty.  \n",
    "  \n",
    "2. Property II:  \n",
    "    1. Uncertainty is additive for independent events.  \n",
    "    2. Let us denote the uncertainty of X as H(X).  \n",
    "    3. Then we wish H(X,Y) = H(X) + H(Y). \n",
    "    4. Consider flippingg two possibly biased but unrelated coins. This condition says that we can either report the outcomes of the coin flips separately or joinly, and we should get the same level of uncertainty either way. It's. very similar to independence.  \n",
    "3. Property III:  \n",
    "    1. Adding an outcome with zero probability has no effect on the uncertainty. \n",
    "    2. Consider distributions A and B.\n",
    "    3. A has two outcomes, one with p=0.2 and one with p=0.8\n",
    "    4. B has three outcomes. One with p=0.2, one with p=0.8, and one with p=0.0\n",
    "    5. We would expect the uncertainty in both cases to be the same.  \n",
    "4. Property IV:\n",
    "    1. Entropy needs to be continuous.\n",
    "    2. Making a small change in probabilities should lead to a small change in uncertainty.\n",
    "    3. We don’t want jumps or holes in our measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7602834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:MLStart] *",
   "language": "python",
   "name": "conda-env-MLStart-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
